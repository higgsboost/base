{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variational autoencoder (VAE)\n",
    "\n",
    "Based on Agustinus Kristladl's blog [link](https://wiseodd.github.io/techblog/2016/12/10/variational-autoencoder/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "VAE is trying to generate data based on some `latent variable`, which is different from vanilla `GANS` that tries to generate data blindly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A game\n",
    "Before we start let's play a little game. Source [link](https://towardsdatascience.com/probability-concepts-explained-marginalisation-2296846344fc).\n",
    "\n",
    "Suppose we have four dice: 4-sided die, 6-sided die, 8-sided die and a 10-sided die (as shown below).\n",
    "\n",
    "The game:\n",
    "\n",
    "- I put a six-sided and an eight-sided die in a red box and a four-sided and ten-sided die in a blue box.\n",
    "- I select a die from each of the red and blue boxes at random and put them in a yellow box.\n",
    "- I select a die at random from the yellow box, roll the die and tell you the result.\n",
    "\n",
    "After playing the game we are told the result is 3. The question that we want to answer is: Did the die most likely come from the red box or the blue box originally?\n",
    "\n",
    "\n",
    "Solution:\n",
    "\n",
    "$$P(dice=3 | box=red) = P(dice=3, die=6 sided | box=red) + P(dice=3, die=8 sided | box=red) = 0.5 * 1/6 + 0.5 * 1/8 = 0.145$$\n",
    "\n",
    "\n",
    "Same for blue ...\n",
    "\n",
    "Notice that we didn't observe the die that we picked. This is marginalization we will be using this fact later on during VAE derivation.\n",
    "\n",
    "The equation is:\n",
    "\n",
    "$$P(X) = \\sum_y  P(X, Y=y)$$\n",
    "\n",
    "then using the rule $P(A|B) = \\frac{P(A\\cap B)}{P(B)}$ gives\n",
    "\n",
    "$$P(X) = \\sum_y  P(X|Y=y)\\times P(Y=y)$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VAE\n",
    "\n",
    "Let's have a joint probability distribution $P(X, z)$ where $X$ and $z$ represents the data and latent variables respectively. If we marginalize the joint distribution then we have : \n",
    "\n",
    "$$P(X) = \\int_z P(X|z) P(z) dz$$\n",
    "\n",
    "The idea of VAE is the infer $P(z)$ using $P(z|X)$. So what is $z$ when given a $X$, or in other words the latent variable needs to understand our data. But what we don't know is $P(z|X)$, so let's approximate it using $Q(z|X)$ using KL divergence.\n",
    "\n",
    "$$D_{KL}[Q(z|X)|P(z|X)] = E[log(Q(z|X) - log(P(z|X))]$$\n",
    "\n",
    "\n",
    "Recal Bayes' theorem: $P(A|B) = \\frac{P(B|A) \\times P(A)}{P(B)}$\n",
    "\n",
    "$$D_{KL}[Q(z|X)|P(z|X)] = E[log(Q(z|X) - log\\frac{P(X|z) \\times P(z)}{P(X)}]$$\n",
    "$$D_{KL}[Q(z|X)|P(z|X)] = E[log(Q(z|X) - log P(X|z) - log P(z) + log P(X)]$$\n",
    "\n",
    "Since $log P(X)$ does not depend on $z$. \n",
    "$$D_{KL}[Q(z|X)|P(z|X)] - log P(X) = E[log(Q(z|X) - log P(X|z) - log P(z) ]$$\n",
    "$$ log P(X) -  D_{KL}[Q(z|X)|P(z|X)] = E[log P(X|z)] -D_{KL}[Q(z|X) |P(z) ]$$\n",
    "\n",
    "\n",
    "So we have $Q(z|X)$ that projects the data $X$ into latent variable space, and $P(X|z)$ that generates the data given the latent space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
