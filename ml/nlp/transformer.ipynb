{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformers\n",
    "13/09/2020\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Differences between RNNS and transformers\n",
    "\n",
    "RNNs:\n",
    "* Long range dependencies (but can be solved by using some attention)\n",
    "* Gradient vanishing and explosion\n",
    "* Large number of training steps\n",
    "* Parallelization computation is also a challenge. With RNNs we have to do computations sequentially so GPUs cannot take advantage of parallelism of GPUs.\n",
    "\n",
    "Transformers:\n",
    "* No recurrence, and facilitate long range dependencies. So long term == short term.\n",
    "* No more gradient vanishing and explosion. Computations for sequences are done together.\n",
    "* Fewer training steps.\n",
    "* No recurrence thus we can take advantage of GPUs.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Attention Mechanism, So what is it? \n",
    "\n",
    "It is kind of like a SELECT in a database. So we retrieve a value $v_i$ for a query $q$ based on a key $k_i$ in a database. Differently attention is uses probabilities thus it is not deterministic. \n",
    "\n",
    "Database example:\n",
    "\n",
    "Database\n",
    "\n",
    "| key | value   |\n",
    "|------|------|\n",
    "|   bob  | 32222|\n",
    "|   dave  | 132|\n",
    "|   john  | 12211|\n",
    "\n",
    "\n",
    "\n",
    "Given a query **dave** then we get **132**.\n",
    "\n",
    "Note: ($\\vec{A}$ means $A$ is a vector, $[B]$ means $B$ is matrix)\n",
    "\n",
    "\n",
    "For attention we have :\n",
    "$$attention(q,[k],[v]) = \\sum_i similarity(q, k_i) \\times v_i$$\n",
    "\n",
    "So for a database\n",
    "\n",
    "```python\n",
    "def similarity(q, k_i):\n",
    "    if q == k_i:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### 1.2.1 What about neural networks?\n",
    "\n",
    "What are some possibilities for NN similarity metrics? So if we have $s_i = f(q,k_i)$ we are some possibilities for $f$?\n",
    "\n",
    "* Dot product: $q^T k_i$ \n",
    "* Scaled dot product ($d$ = dim(key)): $(q^T k_i)/\\sqrt(d)$ \n",
    "* General dot product : $q^T W k_i$\n",
    "* Additive similarity : $w^T_q q + w^T_k k_i$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### References\n",
    "https://www.youtube.com/watch?v=OyFJWRnt_AY\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
