{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient boosted trees\n",
    "\n",
    "Based on Tianqi Chen [link](https://web.njit.edu/~usman/courses/cs675_fall20/BoostedTree.pdf).\n",
    "\n",
    "\n",
    "#### Supervised learning\n",
    "In supervised learning, the $i$-th training example is $x_i \\in \\mathbb{R}^d$, and we are trying to make a prediction $\\hat{y}_i$. Linear models have the format $\\hat{y}_i = \\sum_j w_j x_{ij} $. For linear regression $\\hat{y}_i$ is the predicted score, while in logistic regression $\\frac{1}{1+exp(-\\hat{y}_i))}$ is the predicted probability of the instance $i$ being positive.\n",
    "\n",
    "In linear models we are trying to learn the weights $\\Theta = \\{w_j | j = 1, ..., d\\}$. Thus the objective function is $obj(\\Theta) = L(\\Theta) + \\Omega(\\Theta)$. $L$ is the training loss which can be the square loss $y_i - \\hat{y}_i$, or the logistic loss $y_i \\ln(1 + e^{-\\hat{y}_i}) + (1 - y_i)\\ln(1 + e^{\\hat{y}_i})$ (note previous losses are individual but needs to be sum to get total). The $\\Omega$ function is the regularization function, which measures the complexity of the model, which can be the $L2$ norm $\\lambda ||w||^2$, or $L1$ norm $\\lambda ||w||_1$. Reminder that $||x||_1 = (|x_1| + ... + |x_n|)$.\n",
    "\n",
    "So finally\n",
    "* Ridge regression: $$\\sum_i (y_i - w^T x_i)^2 + \\lambda ||w||^2$$\n",
    "* Lasso: $$\\sum_i (y_i - w^T x_i)^2 + \\lambda ||w||_1$$\n",
    "* Logistic regression: $$\\sum_i [y_i \\ln(1 + e^{-\\hat{y}_i}) + (1 - y_i)\\ln(1 + e^{\\hat{y}_i})] + \\lambda ||w||^2$$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression Tree (A.K.A classification and regression tree CART)\n",
    "\n",
    "Regression trees are similar to decision trees, where at each internal node makes a decision based on features.\n",
    "\n",
    "**Regression tree ensemble** takes the decision of two trees and sums it. The method is invariant to scaling of inputs, so feature normalization is not necessary. It is able to learn higher order interactions between  features. It is also very scalable.\n",
    "\n",
    "So, let us assume that there are $K$ trees\n",
    "\n",
    "$$\n",
    "\\hat{y}_i = \\sum^K_{k=1} f_k(x_i), \\quad f_k \\in \\textit{F} \\quad\n",
    "(1)\n",
    "$$\n",
    "\n",
    "Where $\\textit{F}$ is the space of all regression trees, not that $\\textit{F}$ makes attributes to a score. Parameters are the structures of the tree plus the score in the leaf. The parameters are $\\Theta = \\{f_1, f_2, \\dots, f_K\\}$, so differently from neural networks, we are learning trees.\n",
    "\n",
    "#### Example:\n",
    "\n",
    "Consider a regression tree on time $t$, we want to predict whether someone likes romantic music at time $t$. A regression tree $f_1$ and $f_2$ can be mapped to a piecewise step function over time.\n",
    "\n",
    "Some questions to ask, how do we define the training loss and how do we determine the complexity of the function, i.e. how to we determine how many splitting points we have. \n",
    "\n",
    "The objective is then defined as:\n",
    "\n",
    "$$\n",
    "obj = \\sum^n_{i=1} l(y_i, \\hat{y}_i) + \\sum^K_{k=1} \\Omega(f_k)\n",
    "$$\n",
    "\n",
    "where $\\Omega$ is the complexity of the trees. The loss functions will determine the type of boosting:\n",
    "\n",
    "* **gradient boosted machine** is defined by $l(y_i, \\hat{y}_i)=(y_i-\\hat{y}_i)^2$\n",
    "* **LogitBoost** is defined by $l(y_i, \\hat{y}_i) = y_i \\ln(1 + e^{-\\hat{y}_i}) + (1 - y_i)\\ln(1 + e^{\\hat{y}_i})$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Learning\n",
    "\n",
    "We can not use methods like SGD because they are not vectors but trees, we we have to using *Additive Training* or Boosting. So we start with a constant prediction then add a new function each time.\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\hat{y}_i^{0} &= 0 \\\\\n",
    "\\hat{y}_i^{1} &= f_1(x_i) = \\hat{y}_i^{0} + f_1(x_i) \\\\ \n",
    "\\hat{y}_i^{2} &= f_1(x_i) + f_2(x_i) = \\hat{y}_i^{1} + f_2(x_i) \n",
    "\\\\ \n",
    "\\dots \\\\\n",
    "\\hat{y}_i^{t} &= \\sum^{t}_{k=1} f_k(x_i) = \\hat{y}_i^{t-1} + f_t(x_i) \n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "\n",
    "But how do we decide the to use for $f$ in the above $f_t(x_i)$.\n",
    "\n",
    "So if we have the objective:\n",
    "\n",
    "$$\n",
    "obj = \\sum^n_{i=1} l(y_i, \\hat{y}_i) + \\sum^K_{k=1} \\Omega(f_k)\n",
    "$$\n",
    "\n",
    "If we choose to use gradient boosted machine then we have\n",
    "\n",
    "$$\n",
    "obj = \\sum^n_{i=1} (y_i- (\\hat{y}_i^{t-1} - f_t(x_i)) )^2 + \\sum^K_{k=1} \\Omega(f_k) \\\\\n",
    "obj = \\sum^n_{i=1} (2(\\hat{y}_i^{t-1} - f_t(x_i)) + (\\hat{y}_i^{t-1} - f_t(x_i))^ 2  ) + \\sum^K_{k=1} \\Omega(f_k)\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
